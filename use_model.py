import yfinance as yf
import pandas as pd
import numpy as np
import tensorflow as tf
from keras.models import load_model
import joblib
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨æ–‡ä»¶åï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
SCALER_NAME = "E:\Stock_114_8_9\scaler\enhanced_model_scaler.pkl"
MODEL_NAME = "E:\Stock_114_8_9\model\enhanced_model.h5"

class stockStockPredictor:
    def __init__(self, scaler_path=None, model_path=None):
        """åˆå§‹åŒ–é æ¸¬å™¨"""
        self.scaler_path = scaler_path or SCALER_NAME
        self.model_path = model_path or MODEL_NAME
        self.scaler = None
        self.model = None
        self.feature_names = self._get_feature_names()
        
    def _get_feature_names(self):
        """ç²å–ç‰¹å¾µåç¨±åˆ—è¡¨ï¼ˆéœ€è¦èˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰"""
        return [
            # åƒ¹æ ¼ç‰¹å¾µ
            'Returns_1', 'Returns_3', 'Returns_5', 'Returns_10',
            
            # ç§»å‹•å¹³å‡ç³»çµ±
            'MA5', 'MA10', 'MA20', 'MA50', 'MA100',
            'EMA5', 'EMA10', 'EMA20', 'EMA50',
            'Price_MA5_ratio', 'Price_MA20_ratio', 'Price_MA50_ratio',
            'Price_EMA10_ratio', 'Price_EMA20_ratio',
            'MA5_slope', 'MA10_slope', 'MA20_slope',
            'MA5_MA20_cross', 'MA10_MA50_cross', 'MA20_MA100_cross',
            
            # å¤šåƒæ•¸æŠ€è¡“æŒ‡æ¨™
            'RSI7', 'RSI14', 'RSI21', 'RSI28',
            'RSI14_signal', 'RSI14_divergence',
            'MACD_12_26_9', 'MACD_signal_12_26_9', 'MACD_histogram_12_26_9', 'MACD_cross_12_26_9',
            'MACD_5_35_5', 'MACD_19_39_9',
            
            # å¸ƒæ—å¸¶ç³»çµ±
            'BB_width_20_2', 'BB_position_20_2', 'BB_squeeze_20_2',
            'BB_width_20_2.5', 'BB_width_10_1.5',
            
            # KDJç³»çµ±
            'K_9_3_3', 'D_9_3_3', 'J_9_3_3', 'KD_cross_9_3_3',
            'K_14_5_5', 'D_14_5_5',
            
            # å…¶ä»–æŠ€è¡“æŒ‡æ¨™
            'Williams_R', 'CCI', 'Stoch_RSI', 'ADX',
            
            # æˆäº¤é‡åˆ†æ
            'Volume_ratio', 'Volume_trend', 'Volume_price_trend', 
            'Price_Volume_correlation', 'OBV_signal',
            
            # æ³¢å‹•ç‡ç‰¹å¾µ
            'ATR7_ratio', 'ATR14_ratio', 'ATR21_ratio', 'ATR30_ratio',
            'Volatility7', 'Volatility14', 'Volatility21', 'Volatility30',
            
            # ä½ç½®å’Œå½¢æ…‹
            'High_Low_ratio', 'Close_position', 'Upper_shadow', 'Lower_shadow',
            'Doji', 'Hammer', 'Engulfing',
            
            # è¶¨å‹¢åˆ†æ
            'Trend_strength', 'Price_velocity', 'Price_acceleration',
            'Resistance_distance_10', 'Resistance_distance_20', 'Resistance_distance_50',
            'Support_distance_10', 'Support_distance_20', 'Support_distance_50',
            
            # SOXç›¸é—œç‰¹å¾µ
            'SOX_Returns_1', 'SOX_Returns_5', 'SOX_RSI', 'SOX_Momentum',
            'SOX_Volatility', 'stock_SOX_Corr', 'stock_SOX_Ratio', 'stock_vs_SOX',
            
            # å°è‚¡æŒ‡æ•¸ç‰¹å¾µ
            'TWII_Returns_1', 'TWII_Returns_5', 'TWII_RSI', 'TWII_Momentum',
            'TWII_Volatility', 'stock_TWII_Corr', 'stock_TWII_Ratio', 'stock_vs_TWII',
            
            # åŒ¯ç‡ç›¸é—œç‰¹å¾µ
            'USD_Returns_1', 'USD_Volatility', 'stock_USD_Corr',
            'USDTWD_Returns_1', 'stock_TW_Corr'
        ]
    
    def load_model_and_scaler(self):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨"""
        try:
            print("æ­£åœ¨è¼‰å…¥æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨...")
            self.scaler = joblib.load(self.scaler_path)
            
            # å®šç¾©è‡ªå®šç¾©å‡½æ•¸ä»¥è§£æ±ºè¼‰å…¥å•é¡Œ
            def apply_attention_weight(inputs):
                features, weight = inputs
                weight_expanded = tf.expand_dims(weight, axis=1)
                return features * weight_expanded
            
            # ä½¿ç”¨custom_objectsè¼‰å…¥æ¨¡å‹
            custom_objects = {
                'apply_attention_weight': apply_attention_weight
            }
            
            try:
                # æ–¹æ³•1: ä½¿ç”¨custom_objects
                self.model = load_model(self.model_path, custom_objects=custom_objects)
                print("ä½¿ç”¨custom_objectsæˆåŠŸè¼‰å…¥æ¨¡å‹ï¼")
            except Exception as e1:
                try:
                    # æ–¹æ³•2: åªè¼‰å…¥æ¬Šé‡
                    print("å˜—è©¦æ›¿ä»£è¼‰å…¥æ–¹æ³•...")
                    # éœ€è¦é‡æ–°å‰µå»ºæ¨¡å‹æ¶æ§‹ç„¶å¾Œè¼‰å…¥æ¬Šé‡
                    self.model = self._load_model_weights_only()
                    print("ä½¿ç”¨æ¬Šé‡è¼‰å…¥æ–¹æ³•æˆåŠŸï¼")
                except Exception as e2:
                    print(f"æ‰€æœ‰è¼‰å…¥æ–¹æ³•éƒ½å¤±æ•—:")
                    print(f"æ–¹æ³•1éŒ¯èª¤: {str(e1)}")
                    print(f"æ–¹æ³•2éŒ¯èª¤: {str(e2)}")
                    return False
            
            print("æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸï¼")
            return True
        except Exception as e:
            print(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")
            print("\nå¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
            print("1. é‡æ–°è¨“ç·´æ¨¡å‹ä¸¦ä¿å­˜")
            print("2. ä½¿ç”¨model.save_weights()ä¿å­˜æ¬Šé‡è€Œä¸æ˜¯å®Œæ•´æ¨¡å‹")
            print("3. ç¢ºèªæ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§")
            return False
    
    def _load_model_weights_only(self):
        """åªè¼‰å…¥æ¨¡å‹æ¬Šé‡çš„æ–¹æ³•ï¼ˆéœ€è¦é‡å»ºæ¶æ§‹ï¼‰"""
        print("æ­£åœ¨é‡å»ºæ¨¡å‹æ¶æ§‹...")
        
        # é€™è£¡éœ€è¦é‡æ–°å‰µå»ºèˆ‡è¨“ç·´æ™‚ç›¸åŒçš„æ¨¡å‹æ¶æ§‹
        # ç”±æ–¼åŸå§‹æ¶æ§‹æ¯”è¼ƒè¤‡é›œï¼Œæˆ‘å€‘å‰µå»ºä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬
        from keras.models import Sequential
        from keras.layers import LSTM, Dense, Dropout, Bidirectional
        
        # å‰µå»ºç°¡åŒ–çš„æ¨¡å‹æ¶æ§‹ï¼ˆç”¨æ–¼ç·Šæ€¥æƒ…æ³ï¼‰
        model = Sequential([
            Bidirectional(LSTM(64, return_sequences=True, input_shape=(30, len(self.feature_names)))),
            Dropout(0.3),
            Bidirectional(LSTM(32, return_sequences=False)),
            Dropout(0.3),
            Dense(128, activation='relu'),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.2),
            Dense(3, activation='softmax')
        ])
        
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        # å˜—è©¦è¼‰å…¥æ¬Šé‡
        try:
            # å¦‚æœæœ‰æ¬Šé‡æ–‡ä»¶
            weights_path = self.model_path.replace('.h5', '_weights.h5')
            model.load_weights(weights_path)
            print("æˆåŠŸè¼‰å…¥æ¬Šé‡æ–‡ä»¶ï¼")
            return model
        except:
            # å¦‚æœæ²’æœ‰å–®ç¨çš„æ¬Šé‡æ–‡ä»¶ï¼Œå˜—è©¦å¾å®Œæ•´æ¨¡å‹æå–æ¬Šé‡
            print("è­¦å‘Š: ç„¡æ³•è¼‰å…¥åŸå§‹æ¨¡å‹ï¼Œè«‹è€ƒæ…®é‡æ–°è¨“ç·´")
            return None
    
    def safe_divide(self, numerator, denominator):
        """å®‰å…¨é™¤æ³•"""
        if isinstance(numerator, np.ndarray):
            numerator = pd.Series(numerator)
        if isinstance(denominator, np.ndarray):
            denominator = pd.Series(denominator)
        
        result = numerator / (denominator.abs() + 1e-6)
        result = result.replace([np.inf, -np.inf], np.nan)
        return result
    
    def compute_RSI(self, series, period=14):
        """è¨ˆç®—RSIæŒ‡æ¨™"""
        delta = series.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        avg_gain = gain.rolling(period).mean()
        avg_loss = loss.rolling(period).mean()
        rs = self.safe_divide(avg_gain, avg_loss)
        rsi = 100 - self.safe_divide(100, (1 + rs))
        return rsi.fillna(50)
    
    def compute_MACD(self, series, fast=12, slow=26, signal=9):
        """è¨ˆç®—MACDæŒ‡æ¨™"""
        ema_fast = series.ewm(span=fast).mean()
        ema_slow = series.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal).mean()
        histogram = macd - signal_line
        return macd, signal_line, histogram
    
    def compute_bollinger_bands(self, series, period=20, std_dev=2):
        """è¨ˆç®—å¸ƒæ—å¸¶"""
        ma = series.rolling(period).mean()
        std = series.rolling(period).std()
        upper = ma + (std * std_dev)
        lower = ma - (std * std_dev)
        return upper, lower, ma
    
    def compute_KDJ(self, df, n=9, k_period=3, d_period=3):
        """è¨ˆç®—KDJæŒ‡æ¨™"""
        low_min = df['Low'].rolling(n).min()
        high_max = df['High'].rolling(n).max()
        rsv = 100 * self.safe_divide((df['Close'] - low_min), (high_max - low_min))
        k = rsv.ewm(com=(k_period - 1), adjust=False).mean()
        d = k.ewm(com=(d_period - 1), adjust=False).mean()
        j = 3 * k - 2 * d
        return k, d, j
    
    def compute_williams_r(self, df, period=14):
        """è¨ˆç®—Williams RæŒ‡æ¨™"""
        high_max = df['High'].rolling(period).max()
        low_min = df['Low'].rolling(period).min()
        wr = -100 * self.safe_divide((high_max - df['Close']), (high_max - low_min))
        return wr
    
    def compute_ATR(self, df, period=14):
        """è¨ˆç®—ATRæŒ‡æ¨™"""
        high_low = df['High'] - df['Low']
        high_close = np.abs(df['High'] - df['Close'].shift())
        low_close = np.abs(df['Low'] - df['Close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        return true_range.rolling(period).mean()
    
    def compute_obv(self, df):
        """è¨ˆç®—OBVæŒ‡æ¨™"""
        obv = [0]
        for i in range(1, len(df)):
            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                obv.append(obv[-1] + df['Volume'].iloc[i])
            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                obv.append(obv[-1] - df['Volume'].iloc[i])
            else:
                obv.append(obv[-1])
        return pd.Series(obv, index=df.index)
    
    def fetch_latest_data(self, end_time, days_back=200):
        """ç²å–æœ€æ–°çš„å¸‚å ´æ•¸æ“š"""
        print("æ­£åœ¨ä¸‹è¼‰æœ€æ–°å¸‚å ´æ•¸æ“š...")
        end_date = end_time.strftime('%Y-%m-%d')
        start_date = (end_time - timedelta(days=days_back)).strftime('%Y-%m-%d')
        print(end_time)
        try:
            # å°ç©é›»æ•¸æ“š
            stock_data = yf.download("2330.TW", start=start_date, end=end_date)
            if isinstance(stock_data.columns, pd.MultiIndex):
                stock_data.columns = stock_data.columns.get_level_values(0)
            
            # SOXæŒ‡æ•¸
            sox_data = yf.download("SOXX", start=start_date, end=end_date)
            if isinstance(sox_data.columns, pd.MultiIndex):
                sox_data.columns = sox_data.columns.get_level_values(0)
            
            # å°è‚¡åŠ æ¬ŠæŒ‡æ•¸
            taiwan_index = yf.download("^TWII", start=start_date, end=end_date)
            if isinstance(taiwan_index.columns, pd.MultiIndex):
                taiwan_index.columns = taiwan_index.columns.get_level_values(0)
            
            # ç¾å…ƒæŒ‡æ•¸
            usd_index = yf.download("DX-Y.NYB", start=start_date, end=end_date)
            if isinstance(usd_index.columns, pd.MultiIndex):
                usd_index.columns = usd_index.columns.get_level_values(0)
            
            # å°å¹£åŒ¯ç‡
            try:
                usdtwd = yf.download("USDTWD=X", start=start_date, end=end_date)
            except:
                usdtwd = yf.download("TWD=X", start=start_date, end=end_date)
            
            if isinstance(usdtwd.columns, pd.MultiIndex):
                usdtwd.columns = usdtwd.columns.get_level_values(0)
            
            print("å¸‚å ´æ•¸æ“šä¸‹è¼‰å®Œæˆï¼")
            return stock_data, sox_data, taiwan_index, usd_index, usdtwd
            
        except Exception as e:
            print(f"æ•¸æ“šä¸‹è¼‰å¤±æ•—: {str(e)}")
            return None, None, None, None, None
    
    def compute_features(self, stock_data, sox_data, taiwan_index, usd_index, usdtwd): # 2330åƒ¹éŒ¢ soxåƒ¹éŒ¢ åŠ æ¬ŠæŒ‡æ•¸ ç¾å…ƒæŒ‡æ•¸ å°å¹£æŒ‡æ•¸
        """è¨ˆç®—æ‰€æœ‰ç‰¹å¾µ"""
        print("æ­£åœ¨è¨ˆç®—æŠ€è¡“æŒ‡æ¨™...")
        
        # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½å»é™¤NaN
        for data in [stock_data, sox_data, taiwan_index, usd_index, usdtwd]:
            data.dropna(inplace=True)
        
        # æ‰¾åˆ°å…±åŒçš„æ—¥æœŸç¯„åœ
        common_dates = stock_data.index.intersection(sox_data.index)\
                                      .intersection(taiwan_index.index)\
                                      .intersection(usd_index.index)\
                                      .intersection(usdtwd.index)
        
        if len(common_dates) < 100:
            print(f"è­¦å‘Š: å…±åŒæ—¥æœŸæ•¸é‡ä¸è¶³ ({len(common_dates)})")
        
        # å°é½Šæ‰€æœ‰æ•¸æ“šåˆ°å…±åŒæ—¥æœŸ
        stock_data = stock_data.loc[common_dates]
        sox_data = sox_data.loc[common_dates]
        taiwan_index = taiwan_index.loc[common_dates]
        usd_index = usd_index.loc[common_dates]
        usdtwd = usdtwd.loc[common_dates]

        
        # åˆå§‹åŒ–ç‰¹å¾µDataFrame
        features = pd.DataFrame(index=common_dates)
        
        try:
            # åŸºç¤åƒ¹æ ¼ç‰¹å¾µ
            features['Returns_1'] = stock_data['Close'].pct_change(1)
            features['Returns_3'] = stock_data['Close'].pct_change(3)
            features['Returns_5'] = stock_data['Close'].pct_change(5)
            features['Returns_10'] = stock_data['Close'].pct_change(10)
            
            # ç§»å‹•å¹³å‡
            periods = [5, 10, 20, 50, 100]
            for period in periods:
                features[f'MA{period}'] = stock_data['Close'].rolling(period).mean()
                features[f'EMA{period}'] = stock_data['Close'].ewm(span=period).mean()
                features[f'MA{period}_slope'] = features[f'MA{period}'].diff(5)
                features[f'Price_MA{period}_ratio'] = self.safe_divide(stock_data['Close'], features[f'MA{period}'])
                if period <= 50:  # åªè¨ˆç®—éƒ¨åˆ†EMAæ¯”ç‡
                    features[f'Price_EMA{period}_ratio'] = self.safe_divide(stock_data['Close'], features[f'EMA{period}'])
            
            # MAäº¤å‰ä¿¡è™Ÿ
            features['MA5_MA20_cross'] = np.where(features['MA5'] > features['MA20'], 1, -1)
            features['MA10_MA50_cross'] = np.where(features['MA10'] > features['MA50'], 1, -1)
            features['MA20_MA100_cross'] = np.where(features['MA20'] > features['MA100'], 1, -1)
            
            # RSIæŒ‡æ¨™
            for period in [7, 14, 21, 28]:
                features[f'RSI{period}'] = self.compute_RSI(stock_data['Close'], period)
                if period == 14:
                    features['RSI14_signal'] = np.where(features['RSI14'] > 70, -1, 
                                                       np.where(features['RSI14'] < 30, 1, 0))
                    features['RSI14_divergence'] = 0  # ç°¡åŒ–è™•ç†
            
            # MACDæŒ‡æ¨™
            for fast, slow, signal in [(12, 26, 9), (5, 35, 5), (19, 39, 9)]:
                macd_line, signal_line, histogram = self.compute_MACD(stock_data['Close'], fast, slow, signal)
                suffix = f"_{fast}_{slow}_{signal}"
                features[f'MACD{suffix}'] = macd_line
                if fast == 12:  # åªä¿å­˜æ¨™æº–MACDçš„ä¿¡è™Ÿç·šå’ŒæŸ±ç‹€åœ–
                    features[f'MACD_signal{suffix}'] = signal_line
                    features[f'MACD_histogram{suffix}'] = histogram
                    features[f'MACD_cross{suffix}'] = np.where(macd_line > signal_line, 1, -1)
            
            # å¸ƒæ—å¸¶
            for period, std_dev in [(20, 2), (20, 2.5), (10, 1.5)]:
                bb_upper, bb_lower, bb_middle = self.compute_bollinger_bands(stock_data['Close'], period, std_dev)
                suffix = f"_{period}_{std_dev}"
                features[f'BB_width{suffix}'] = self.safe_divide((bb_upper - bb_lower), bb_middle)
                features[f'BB_position{suffix}'] = self.safe_divide((stock_data['Close'] - bb_lower), (bb_upper - bb_lower))
                if period == 20 and std_dev == 2:
                    features['BB_squeeze_20_2'] = (features['BB_width_20_2'] < features['BB_width_20_2'].rolling(20).mean() * 0.8).astype(int)
            
            # KDJæŒ‡æ¨™
            for n, k_period, d_period in [(9, 3, 3), (14, 5, 5)]:
                k, d, j = self.compute_KDJ(stock_data, n, k_period, d_period)
                suffix = f"_{n}_{k_period}_{d_period}"
                features[f'K{suffix}'] = k
                features[f'D{suffix}'] = d
                if n == 9:
                    features[f'J{suffix}'] = j
                    features[f'KD_cross{suffix}'] = np.where(k > d, 1, -1)
            
            # å…¶ä»–æŠ€è¡“æŒ‡æ¨™
            features['Williams_R'] = self.compute_williams_r(stock_data)
            features['CCI'] = 0  # ç°¡åŒ–è™•ç†
            features['Stoch_RSI'] = 0  # ç°¡åŒ–è™•ç†
            features['ADX'] = 0  # ç°¡åŒ–è™•ç†
            
            # æˆäº¤é‡åˆ†æ
            volume_ma20 = stock_data['Volume'].rolling(20).mean()
            features['Volume_ratio'] = self.safe_divide(stock_data['Volume'], volume_ma20)
            features['Volume_trend'] = stock_data['Volume'].rolling(5).mean() / volume_ma20 - 1
            features['Volume_price_trend'] = np.where(
                (features['Returns_5'] > 0) & (features['Volume_ratio'] > 1.2), 1,
                np.where((features['Returns_5'] < 0) & (features['Volume_ratio'] > 1.2), -1, 0)
            )
            features['Price_Volume_correlation'] = stock_data['Close'].rolling(10).corr(stock_data['Volume']).fillna(0)
            
            # OBVæŒ‡æ¨™
            obv = self.compute_obv(stock_data)
            obv_ma = obv.rolling(20).mean()
            features['OBV_signal'] = np.where(obv > obv_ma, 1, -1)
            
            # ATRå’Œæ³¢å‹•ç‡
            for period in [7, 14, 21, 30]:
                atr = self.compute_ATR(stock_data, period)
                features[f'ATR{period}_ratio'] = self.safe_divide(atr, stock_data['Close'])
                features[f'Volatility{period}'] = features['Returns_1'].rolling(period).std()
            
            # åƒ¹æ ¼ä½ç½®åˆ†æ
            features['High_Low_ratio'] = self.safe_divide((stock_data['High'] - stock_data['Low']), stock_data['Close'])
            features['Close_position'] = self.safe_divide((stock_data['Close'] - stock_data['Low']), (stock_data['High'] - stock_data['Low']))
            features['Upper_shadow'] = self.safe_divide((stock_data['High'] - np.maximum(stock_data['Open'], stock_data['Close'])), stock_data['Close'])
            features['Lower_shadow'] = self.safe_divide((np.minimum(stock_data['Open'], stock_data['Close']) - stock_data['Low']), stock_data['Close'])
            
            # å½¢æ…‹è­˜åˆ¥ï¼ˆç°¡åŒ–ï¼‰
            features['Doji'] = ((stock_data['Close'] - stock_data['Open']).abs() / (stock_data['High'] - stock_data['Low']) < 0.1).astype(int)
            features['Hammer'] = 0  # ç°¡åŒ–è™•ç†
            features['Engulfing'] = 0  # ç°¡åŒ–è™•ç†
            
            # è¶¨å‹¢åˆ†æ
            features['Trend_strength'] = 0.5  # ç°¡åŒ–è™•ç†
            features['Price_velocity'] = stock_data['Close'].diff() / stock_data['Close'].shift()
            features['Price_acceleration'] = features['Price_velocity'].diff()
            
            # æ”¯æ’é˜»åŠ›ä½
            for window in [10, 20, 50]:
                resistance = stock_data['High'].rolling(window).max()
                support = stock_data['Low'].rolling(window).min()
                features[f'Resistance_distance_{window}'] = self.safe_divide((resistance - stock_data['Close']), stock_data['Close'])
                features[f'Support_distance_{window}'] = self.safe_divide((stock_data['Close'] - support), stock_data['Close'])
            
            # å¸‚å ´ç›¸é—œç‰¹å¾µ
            # SOXç›¸é—œ
            sox_returns = sox_data['Close'].pct_change()
            features['SOX_Returns_1'] = sox_returns
            features['SOX_Returns_5'] = sox_data['Close'].pct_change(5)
            features['SOX_RSI'] = self.compute_RSI(sox_data['Close'])
            sox_ma20 = sox_data['Close'].rolling(20).mean()
            features['SOX_Momentum'] = sox_data['Close'] / sox_ma20 - 1
            features['SOX_Volatility'] = sox_returns.rolling(20).std()
            
            # å°è‚¡ç›¸é—œ
            twii_returns = taiwan_index['Close'].pct_change()
            features['TWII_Returns_1'] = twii_returns
            features['TWII_Returns_5'] = taiwan_index['Close'].pct_change(5)
            features['TWII_RSI'] = self.compute_RSI(taiwan_index['Close'])
            twii_ma20 = taiwan_index['Close'].rolling(20).mean()
            features['TWII_Momentum'] = taiwan_index['Close'] / twii_ma20 - 1
            features['TWII_Volatility'] = twii_returns.rolling(20).std()
            
            # ç¾å…ƒå’ŒåŒ¯ç‡
            usd_returns = usd_index['Close'].pct_change()
            twd_returns = usdtwd['Close'].pct_change()
            features['USD_Returns_1'] = usd_returns
            features['USD_Volatility'] = usd_returns.rolling(20).std()
            features['USDTWD_Returns_1'] = twd_returns
            
            # ç›¸é—œæ€§ç‰¹å¾µ
            stock_returns = features['Returns_1']
            features['stock_SOX_Corr'] = stock_returns.rolling(20).corr(sox_returns)
            features['stock_TWII_Corr'] = stock_returns.rolling(20).corr(twii_returns)
            features['stock_USD_Corr'] = stock_returns.rolling(20).corr(usd_returns)
            features['stock_TW_Corr'] = stock_returns.rolling(20).corr(twd_returns)
            
            # ç›¸å°å¼·åº¦
            features['stock_SOX_Ratio'] = self.safe_divide(stock_data['Close'], sox_data['Close'])
            features['stock_TWII_Ratio'] = self.safe_divide(stock_data['Close'], taiwan_index['Close'])
            features['stock_vs_SOX'] = stock_returns - sox_returns
            features['stock_vs_TWII'] = stock_returns - twii_returns
            
            print("æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼")
            
            # æ•¸æ“šæ¸…ç†
            for col in features.columns:
                features[col] = features[col].replace([np.inf, -np.inf], np.nan)
                features[col] = features[col].fillna(method='ffill').fillna(0)
            
            return features # å„ç¨®æŠ€è¡“æŒ‡æ¨™
            
        except Exception as e:
            print(f"ç‰¹å¾µè¨ˆç®—éŒ¯èª¤: {str(e)}")
            return None
    
    def predict_tomorrow(self, end_time, confidence_threshold=0.75, ):
        """é æ¸¬æ˜å¤©çš„è‚¡åƒ¹èµ°å‹¢"""
        if not self.scaler or not self.model:
            print("è«‹å…ˆè¼‰å…¥æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ï¼")
            return None
        
        # ç²å–æœ€æ–°æ•¸æ“š
        stock_data, sox_data, taiwan_index, usd_index, usdtwd = self.fetch_latest_data(end_time=end_time, days_back=200)
        
        if stock_data is None:
            print("ç„¡æ³•ç²å–å¸‚å ´æ•¸æ“šï¼")
            return None
        
        # è¨ˆç®—ç‰¹å¾µ
        features_df = self.compute_features(stock_data, sox_data, taiwan_index, usd_index, usdtwd)
        
        if features_df is None:
            print("ç‰¹å¾µè¨ˆç®—å¤±æ•—ï¼")
            return None
        
        # ç¢ºä¿ç‰¹å¾µèˆ‡è¨“ç·´æ™‚ä¸€è‡´
        existing_features = [f for f in self.feature_names if f in features_df.columns]
        missing_features = [f for f in self.feature_names if f not in features_df.columns]
        
        if missing_features:
            print(f"ç¼ºå°‘ç‰¹å¾µ: {len(missing_features)}å€‹")
            # ç”¨0å¡«å……ç¼ºå°‘çš„ç‰¹å¾µ
            for feature in missing_features:
                features_df[feature] = 0
        
        # é¸æ“‡ç‰¹å¾µä¸¦æ’åº
        feature_data = features_df[self.feature_names].copy()
        print(feature_data)
        # æ¨™æº–åŒ–ç‰¹å¾µ
        try:
            scaled_features = self.scaler.transform(feature_data)
        except Exception as e:
            print(f"ç‰¹å¾µæ¨™æº–åŒ–å¤±æ•—: {str(e)}")
            return None
        
        # å‰µå»ºåºåˆ—ï¼ˆä½¿ç”¨æœ€è¿‘30å¤©çš„æ•¸æ“šï¼‰
        time_steps = 30
        if len(scaled_features) < time_steps:
            print(f"æ•¸æ“šä¸è¶³ï¼Œéœ€è¦è‡³å°‘{time_steps}å¤©çš„æ•¸æ“šï¼")
            return None
        
        # å–æœ€å¾Œ30å¤©ä½œç‚ºé æ¸¬åºåˆ—
        sequence = scaled_features[-time_steps:].reshape(1, time_steps, len(self.feature_names))
        
        # é€²è¡Œé æ¸¬
        try:
            prediction_probs = self.model.predict(sequence, verbose=0)[0]
            predicted_class = np.argmax(prediction_probs)
            confidence = np.max(prediction_probs)
            
            # ç²å–æœ€æ–°æ”¶ç›¤åƒ¹
            latest_close = stock_data['Close'].iloc[-1]
            
            # é¡åˆ¥æ˜ å°„
            class_names = ['è·Œ', 'è§€æœ›', 'æ¼²']
            prediction_name = class_names[predicted_class]
            
            # æº–å‚™çµæœ
            result = {
                'prediction': prediction_name,
                'prediction_class': int(predicted_class),
                'confidence': float(confidence),
                'probabilities': {
                    'è·Œ': float(prediction_probs[0]),
                    'è§€æœ›': float(prediction_probs[1]),
                    'æ¼²': float(prediction_probs[2])
                },
                'latest_close': float(latest_close),
                'prediction_date': end_time.strftime('%Y-%m-%d'),
                'data_date': stock_data.index[-1].strftime('%Y-%m-%d'),
                'high_confidence': bool(confidence > confidence_threshold)
            }
            
            return result
            
        except Exception as e:
            print(f"é æ¸¬å¤±æ•—: {str(e)}")
            return None
    
    def print_prediction_report(self, result):
        """æ‰“å°é æ¸¬å ±å‘Š"""
        if not result:
            print("ç„¡é æ¸¬çµæœå¯é¡¯ç¤º")
            return
        
        print("\n" + "="*50)
        print("ğŸ›ï¸  å°ç©é›»è‚¡åƒ¹é æ¸¬å ±å‘Š")
        print("="*50)
        
        print(f"ğŸ“… é æ¸¬æ—¥æœŸ: {result['prediction_date']}")
        print(f"ğŸ“Š æ•¸æ“šæˆªè‡³: {result['data_date']}")
        print(f"ğŸ’° æœ€æ–°æ”¶ç›¤åƒ¹: NT$ {result['latest_close']:.2f}")
        
        print(f"\nğŸ”® æ˜å¤©é æ¸¬: {result['prediction']}")
        print(f"ğŸ“ˆ é æ¸¬ç½®ä¿¡åº¦: {result['confidence']:.1%}")
        print(f"âš¡ é«˜ç½®ä¿¡åº¦é æ¸¬: {'æ˜¯' if result['high_confidence'] else 'å¦'}")
        
        print("\nğŸ“Š å„é¡åˆ¥æ©Ÿç‡:")
        for class_name, prob in result['probabilities'].items():
            emoji = "ğŸ“ˆ" if class_name == "æ¼²" else "ğŸ“‰" if class_name == "è·Œ" else "â¸ï¸"
            print(f"  {emoji} {class_name}: {prob:.1%}")
        
        print(f"\nğŸ’¡ æŠ•è³‡å»ºè­°:")
        if result['high_confidence']:
            if result['prediction'] == 'æ¼²':
                print("  ğŸŸ¢ æ¨¡å‹é¡¯ç¤ºé«˜ç½®ä¿¡åº¦çœ‹æ¼²ï¼Œå¯è€ƒæ…®é©ç•¶è²·å…¥")
            elif result['prediction'] == 'è·Œ':
                print("  ğŸ”´ æ¨¡å‹é¡¯ç¤ºé«˜ç½®ä¿¡åº¦çœ‹è·Œï¼Œå»ºè­°è¬¹æ…æ“ä½œ")
            else:
                print("  ğŸŸ¡ æ¨¡å‹å»ºè­°è§€æœ›ï¼Œç­‰å¾…æ›´æ˜ç¢ºä¿¡è™Ÿ")
        else:
            print("  âš ï¸  æ¨¡å‹ç½®ä¿¡åº¦è¼ƒä½ï¼Œå»ºè­°çµåˆå…¶ä»–åˆ†æ")
        
        print("\nâš ï¸  é¢¨éšªæé†’:")
        print("  â€¢ æœ¬é æ¸¬åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°")
        print("  â€¢ è‚¡å¸‚æœ‰é¢¨éšªï¼ŒæŠ•è³‡éœ€è¬¹æ…")
        print("  â€¢ è«‹çµåˆåŸºæœ¬é¢åˆ†æå’Œå¸‚å ´ç’°å¢ƒåˆ¤æ–·")
        print("="*50)

    def join_txt2msg(self, result):
        msg=""
        msg+="ğŸŒ• Lymetix ä¾‹è¡Œå ±å‘Š\n\n"
        msg+="ğŸ›ï¸ å°ç©é›»è‚¡åƒ¹é æ¸¬å ±å‘Š"
        
        msg+=f"\nğŸ“… é æ¸¬æ—¥æœŸ: {result['prediction_date']}"
        msg+=f"\nğŸ’° æœ€æ–°æ”¶ç›¤åƒ¹: NT$ {result['latest_close']:.2f}"
        
        msg+=f"\n\nğŸ”® ä»Šå¤©é æ¸¬: {result['prediction']}"
        msg+=f"\nğŸ“ˆ é æ¸¬ç½®ä¿¡åº¦: {result['confidence']:.1%}"
        msg+=f"\nâš¡ é«˜ç½®ä¿¡åº¦é æ¸¬: {'æ˜¯' if result['high_confidence'] else 'å¦'}"
        
        msg += "\n\nğŸ“Š å„é¡åˆ¥æ©Ÿç‡:"
        for class_name, prob in result['probabilities'].items():
            emoji = "ğŸ“ˆ" if class_name == "æ¼²" else "ğŸ“‰" if class_name == "è·Œ" else "â¸ï¸"
            msg+=f"\n  â€¢ {emoji} {class_name}: {prob:.1%}"
        
        msg+="\n\nğŸ’¡ æŠ•è³‡å»ºè­°:"
        if result['high_confidence']:
            if result['prediction'] == 'æ¼²':
                msg+="\n  â€¢ æ¨¡å‹é¡¯ç¤ºé«˜ç½®ä¿¡åº¦çœ‹æ¼²ï¼Œå¯è€ƒæ…®é©ç•¶è²·å…¥"
            elif result['prediction'] == 'è·Œ':
                msg+="\n  â€¢ æ¨¡å‹é¡¯ç¤ºé«˜ç½®ä¿¡åº¦çœ‹è·Œï¼Œå»ºè­°è¬¹æ…æ“ä½œ"
            else:
                msg+="\n  â€¢ æ¨¡å‹å»ºè­°è§€æœ›ï¼Œç­‰å¾…æ›´æ˜ç¢ºä¿¡è™Ÿ"
        else:
            msg+="\n  âš ï¸  æ¨¡å‹ç½®ä¿¡åº¦è¼ƒä½ï¼Œå»ºè­°çµåˆå…¶ä»–åˆ†æ"
        
        msg+="\n\nâš ï¸  é¢¨éšªæé†’:"
        msg+="\n  â€¢ æœ¬é æ¸¬åƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°"
        msg+="\n  â€¢ è‚¡å¸‚æœ‰é¢¨éšªï¼ŒæŠ•è³‡éœ€è¬¹æ…"
        msg+="\n  â€¢ è«‹çµåˆåŸºæœ¬é¢åˆ†æå’Œå¸‚å ´ç’°å¢ƒåˆ¤æ–·"
        return msg

    def backtest_predictions(self, end_time, days_back=4000):
        if not self.scaler or not self.model:
            print("è«‹å…ˆè¼‰å…¥æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ï¼")
            return None
        
        all_results = []
        
        
        # ç²å–å›æ¸¬æ‰€éœ€çš„å…¨éƒ¨æ•¸æ“š
        # é€™è£¡å‡è¨­æ‚¨çš„ fetch_latest_data å‡½å¼å¯ä»¥è™•ç†è¼ƒé•·çš„ period
        # å¦‚æœæ‚¨çš„ fetch_latest_data å‡½å¼ç„¡æ³•è™•ç†ï¼Œæ‚¨éœ€è¦ä¿®æ”¹å®ƒ
        stock_data, sox_data, taiwan_index, usd_index, usdtwd = self.fetch_latest_data(
            end_time=end_time,
            days_back=days_back # ç¢ºä¿å–å¾—è¶³å¤ çš„æ•¸æ“šä¾†è¨ˆç®—ç‰¹å¾µ
        )
        
        if stock_data is None:
            print("ç„¡æ³•ç²å–å¸‚å ´æ•¸æ“šï¼")
            return None
        # è¨ˆç®—æ‰€æœ‰æ•¸æ“šçš„ç‰¹å¾µ
        features_df = self.compute_features(stock_data, sox_data, taiwan_index, usd_index, usdtwd)
        print(features_df)
        
        if features_df is None:
            print("ç‰¹å¾µè¨ˆç®—å¤±æ•—ï¼")
            return None
            
        # ç¢ºä¿ç‰¹å¾µèˆ‡è¨“ç·´æ™‚ä¸€è‡´
        existing_features = [f for f in self.feature_names if f in features_df.columns]
        missing_features = [f for f in self.feature_names if f not in features_df.columns]
        
        if missing_features:
            print(f"ç¼ºå°‘ç‰¹å¾µ: {len(missing_features)}å€‹")
            for feature in missing_features:
                features_df[feature] = 0
                
        feature_data = features_df[self.feature_names].copy()
        
        # æ¨™æº–åŒ–æ‰€æœ‰ç‰¹å¾µ
        try:
            scaled_features = self.scaler.transform(feature_data)
        except Exception as e:
            print(f"ç‰¹å¾µæ¨™æº–åŒ–å¤±æ•—: {str(e)}")
            return None
            
        time_steps = 30
        if len(scaled_features) < time_steps:
            print(f"æ•¸æ“šä¸è¶³ï¼Œéœ€è¦è‡³å°‘{time_steps}å¤©çš„æ•¸æ“šï¼")
            return None
        
        # æ ¸å¿ƒå›æ¸¬è¿´åœˆ
        # å¾ç¬¬ time_steps å¤©é–‹å§‹ï¼Œæ¯å¤©éƒ½é€²è¡Œä¸€æ¬¡é æ¸¬
        for i in range(time_steps, len(scaled_features)):
            print("æ­£åœ¨é æ¸¬æ—¥æœŸ:", stock_data.index[i+146].strftime('%Y-%m-%d'), i)
            # å–å¾—ç•¶å‰æ—¥æœŸçš„åºåˆ—æ•¸æ“š
            sequence = scaled_features[i - time_steps : i].reshape(1, time_steps, len(self.feature_names))
            
            # é€²è¡Œé æ¸¬
            try:
                prediction_probs = self.model.predict(sequence, verbose=0)[0]

                # ç²å–å¯¦éš›èµ°å‹¢
                actual_close_yesterday = stock_data['Close'].iloc[i +146 - 1]
                actual_close_today = stock_data['Close'].iloc[i+146]

                actual_class = -1
                
                # è¨­å®šæ¼²è·Œé–¾å€¼ç‚º 1.5%
                price_change_threshold_percent = 0.015
                price_change_percent = (actual_close_today - actual_close_yesterday) / actual_close_yesterday
                
                if price_change_percent > price_change_threshold_percent:
                    actual_class = 2
                elif price_change_percent < -price_change_threshold_percent:
                    actual_class = 0
                else:
                    actual_class = 1
                
                result = {
                    "date": stock_data.index[i+146].strftime('%Y-%m-%d'),
                    'probabilities': {
                        'è·Œ': float(prediction_probs[0]),
                        'è§€æœ›': float(prediction_probs[1]),
                        'æ¼²': float(prediction_probs[2])
                    },
                    "predict_class": int(np.argmax(prediction_probs)),
                    'actual_class': actual_class,
                    "actual_close_today": float(actual_close_today),
                }
                all_results.append(result)
                
            except Exception as e:
                print(f"æ—¥æœŸ {stock_data.index[i+146].strftime('%Y-%m-%d')} é æ¸¬å¤±æ•—: {str(e)}")
                continue # é‡åˆ°éŒ¯èª¤æ™‚è·³éï¼Œç¹¼çºŒä¸‹ä¸€æ¬¡è¿´åœˆ
        
        return all_results


def main():
    """ä¸»ç¨‹åºç¤ºä¾‹"""
    print("ğŸš€ å°ç©é›»è‚¡åƒ¹é æ¸¬ç³»çµ±å•Ÿå‹•")
    
    # å‰µå»ºé æ¸¬å™¨å¯¦ä¾‹
    predictor = stockStockPredictor()
    
    # è¼‰å…¥æ¨¡å‹
    if not predictor.load_model_and_scaler():
        print("âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œè«‹ç¢ºèªæ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        print("éœ€è¦çš„æ–‡ä»¶:")
        print(f"  - {SCALER_NAME}")
        print(f"  - {MODEL_NAME}")
        return
    
    # é€²è¡Œé æ¸¬
    print("\næ­£åœ¨é€²è¡Œé æ¸¬åˆ†æ...")

    result = predictor.predict_tomorrow(confidence_threshold=0.6, end_time=datetime.now())
    
    if result:
        # æ‰“å°é æ¸¬å ±å‘Š
        predictor.print_prediction_report(result)
        analyzer = AdvancedAnalyzer(predictor)
            
            
        # é¢¨éšªè©•ä¼°
        analyzer.risk_assessment(result)
        
        print(f"\nâœ… é æ¸¬å®Œæˆï¼å»ºè­°çµåˆåŸºæœ¬é¢åˆ†æåšæœ€çµ‚æ±ºç­–ã€‚")
        return result
    else:
        print("âŒ é æ¸¬å¤±æ•—")
        return None

def make_test_report():
    predictor = stockStockPredictor()
    predictor.load_model_and_scaler()
    data = predictor.backtest_predictions(datetime.now(), days_back=6000)
    with open("result_data/back_train.json", "w", encoding="utf-8") as f:
        import json
        json.dump(data, f, ensure_ascii=False, indent=4)  
    

class AdvancedAnalyzer:
    """é€²éšåˆ†æå·¥å…·"""
    
    def __init__(self, predictor):
        self.predictor = predictor
    
    def analyze_prediction_history(self, days=10):
        """åˆ†æè¿‘æœŸé æ¸¬è¶¨å‹¢"""
        print(f"\nğŸ“Š è¿‘{days}å¤©é æ¸¬è¶¨å‹¢åˆ†æ")
        print("æ³¨æ„: æ­¤åŠŸèƒ½éœ€è¦æ­·å²é æ¸¬æ•¸æ“š")
        # é€™è£¡å¯ä»¥æ“´å±•å¯¦ç¾æ­·å²é æ¸¬è¿½è¸ªåŠŸèƒ½
        
    def risk_assessment(self, result):
        """é¢¨éšªè©•ä¼°"""
        if not result:
            return
            
        print("\nâš ï¸  é¢¨éšªè©•ä¼°:")
        
        # åŸºæ–¼ç½®ä¿¡åº¦çš„é¢¨éšªè©•ä¼°
        if result['confidence'] > 0.8:
            risk_level = "ä½"
            color = "ğŸŸ¢"
        elif result['confidence'] > 0.6:
            risk_level = "ä¸­"
            color = "ğŸŸ¡"
        else:
            risk_level = "é«˜"
            color = "ğŸ”´"
        
        print(f"  {color} é æ¸¬é¢¨éšªç­‰ç´š: {risk_level}")
        
        # æ©Ÿç‡åˆ†æ•£åº¦åˆ†æ
        probs = list(result['probabilities'].values())
        entropy = -sum(p * np.log(p + 1e-8) for p in probs)
        max_entropy = np.log(3)  # 3å€‹é¡åˆ¥çš„æœ€å¤§ç†µ
        uncertainty = entropy / max_entropy
        
        print(f"  ğŸ“Š é æ¸¬ä¸ç¢ºå®šæ€§: {uncertainty:.1%}")
        
        if uncertainty > 0.8:
            print("  âš ï¸  æ¨¡å‹å°å„ç¨®å¯èƒ½æ€§éƒ½ä¸å¤ªç¢ºå®šï¼Œå»ºè­°è¬¹æ…æ“ä½œ")
        elif uncertainty < 0.3:
            print("  âœ… æ¨¡å‹é æ¸¬ç›¸å°æ˜ç¢º")
        
        # å»ºè­°å€‰ä½å¤§å°
        if result['high_confidence'] and uncertainty < 0.5:
            if result['prediction'] in ['æ¼²', 'è·Œ']:
                suggested_position = "å¯è€ƒæ…®é©ä¸­å€‰ä½ (20-40%)"
            else:
                suggested_position = "å»ºè­°è§€æœ›æˆ–è¼•å€‰ (5-10%)"
        else:
            suggested_position = "å»ºè­°è¼•å€‰æˆ–è§€æœ› (5-15%)"
        
        print(f"  ğŸ’¼ å»ºè­°å€‰ä½: {suggested_position}")


def convert_model_to_weights_only(original_model_path):
    """å°‡å®Œæ•´æ¨¡å‹è½‰æ›ç‚ºåƒ…æ¬Šé‡æ ¼å¼"""
    try:
        print("ğŸ”„ å˜—è©¦è½‰æ›æ¨¡å‹ç‚ºæ¬Šé‡æ ¼å¼...")
        
        # å®šç¾©è‡ªå®šç¾©å‡½æ•¸
        def apply_attention_weight(inputs):
            features, weight = inputs
            weight_expanded = tf.expand_dims(weight, axis=1)
            return features * weight_expanded
        
        custom_objects = {'apply_attention_weight': apply_attention_weight}
        
        # å˜—è©¦è¼‰å…¥åŸæ¨¡å‹
        model = load_model(original_model_path, custom_objects=custom_objects)
        
        # ä¿å­˜æ¬Šé‡
        weights_path = original_model_path.replace('.h5', '.weights.h5')
        model.save_weights(weights_path)
        print(f"âœ… æ¬Šé‡å·²ä¿å­˜ç‚º: {weights_path}")
        
        # ä¿å­˜æ¨¡å‹æ¶æ§‹(JSONæ ¼å¼)
        architecture_path = original_model_path.replace('.h5', '_architecture.json')
        with open(architecture_path, 'w') as f:
            f.write(model.to_json())
        print(f"ğŸ“ æ¨¡å‹æ¶æ§‹å·²ä¿å­˜ç‚º: {architecture_path}")
        
        return True, weights_path, architecture_path
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è½‰æ›å¤±æ•—: {str(e)}")
        return False, None, None

def make_predict_report(from_date=[2025, 8, 14], back_days=120):
    # å‰µå»ºé æ¸¬å™¨å¯¦ä¾‹
    predictor = stockStockPredictor()
    
    # è¼‰å…¥æ¨¡å‹
    if not predictor.load_model_and_scaler():
        print("âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹ï¼Œè«‹ç¢ºèªæ¨¡å‹æ–‡ä»¶å­˜åœ¨")
        print("éœ€è¦çš„æ–‡ä»¶:")
        print(f"  - {SCALER_NAME}")
        print(f"  - {MODEL_NAME}")
        return
    
    # é€²è¡Œé æ¸¬
    print("\næ­£åœ¨é€²è¡Œé æ¸¬åˆ†æ...")
    all_result = []
    for time in range(1, 0, -1):
        print(f"  é æ¸¬éå»{time}å¤©çš„èµ°å‹¢...")
        import datetime as dt
        end_time = dt.date(from_date[0],from_date[1],from_date[2]) - timedelta(days=time)
        result = predictor.predict_tomorrow(confidence_threshold=0.6, end_time=end_time)
        if result:
            all_result.append(result)
    print(all_result)
    with open("éå»30å¤©é æ¸¬å ±å‘Š_è·Œ.txt", "w", encoding="utf-8") as f:
        for item in all_result:
            f.write(str(item["probabilities"]["è·Œ"])+ "\n")
    with open("éå»30å¤©é æ¸¬å ±å‘Š_è§€æœ›.txt", "w", encoding="utf-8") as f:
        for item in all_result:
            f.write(str(item["probabilities"]["è§€æœ›"])+ "\n")
    with open("éå»30å¤©é æ¸¬å ±å‘Š_æ¼².txt", "w", encoding="utf-8") as f:
        for item in all_result:
            f.write(str(item["probabilities"]["æ¼²"])+ "\n")
    with open("éå»30å¤©é æ¸¬å ±å‘Š_æ—¥æœŸ.txt", "w", encoding="utf-8") as f:
        for item in all_result:
            f.write(str(item["prediction_date"])+ "\n")
    with open("éå»30å¤©é æ¸¬å ±å‘Š_å¯¦éš›.txt", "w", encoding="utf-8") as f:
        for item in all_result:
            f.write(str(item["latest_close"])+ "\n")



if __name__ == "__main__":
    # é¦–å…ˆå˜—è©¦ä¿®å¾©æ¨¡å‹è¼‰å…¥å•é¡Œ
    print("ğŸš€ å°ç©é›»è‚¡åƒ¹é æ¸¬ç³»çµ±å•Ÿå‹•")
    print("æ­£åœ¨æª¢æŸ¥æ¨¡å‹è¼‰å…¥å•é¡Œ...")
    
    # å˜—è©¦ä¿®å¾©æ¨¡å‹è¼‰å…¥å•é¡Œ
    try:
        # æ–¹æ³•1: å˜—è©¦è½‰æ›ç¾æœ‰æ¨¡å‹
        success, weights_path, arch_path = convert_model_to_weights_only(MODEL_NAME)
        if success:
            print("âœ… æ¨¡å‹è½‰æ›æˆåŠŸï¼")
        else:
            print("âŒ æ¨¡å‹è½‰æ›æˆåŠŸï¼")
    except:
        print("âš ï¸ æ¨¡å‹ä¿®å¾©å¤±æ•—ï¼Œå°‡ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")


    result = main()
    # make_test_report()
    
    
    # precision é æ¸¬ä¸­å¤šå°‘æ­£ç¢º  recall æ­£ç¢ºä¸­å¤šå°‘é æ¸¬ f1-score   support
    